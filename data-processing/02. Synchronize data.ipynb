{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T11:16:14.857283Z",
     "start_time": "2018-02-04T11:16:14.693172Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from settings import Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T11:16:15.085412Z",
     "start_time": "2018-02-04T11:16:15.076839Z"
    }
   },
   "outputs": [],
   "source": [
    "paths = Paths(\n",
    "    '/media/blazaid/Saca/Phd/data',\n",
    "    'raw_csvs', 'synced_csvs', 'tmp'\n",
    ")\n",
    "\n",
    "TIME = 'time'  # The name of the time column which will be used for syncing\n",
    "SUBJECTS = ['miguel']\n",
    "DATASETS = ['train', 'test']\n",
    "SENSORS = ['can', 'gps-position', 'gps-speed', 'kinect-image', 'lidar']\n",
    "\n",
    "def ds_filename(subject, dataset, sensor, extension):\n",
    "    return '-'.join([subject, dataset, sensor]) + '.' + extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the dataframes into memory to ease the work. They are not to heavy because the binary data (pointclouds and images) have been saved apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T11:16:16.673112Z",
     "start_time": "2018-02-04T11:16:16.103920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subject miguel\n",
      "\tLoading dataset train\n",
      "\t\tLoading sensor can ... done\n",
      "\t\tLoading sensor gps-position ... done\n",
      "\t\tLoading sensor gps-speed ... done\n",
      "\t\tLoading sensor kinect-image ... done\n",
      "\t\tLoading sensor lidar ... done\n",
      "\tLoading dataset test\n",
      "\t\tLoading sensor can ... done\n",
      "\t\tLoading sensor gps-position ... done\n",
      "\t\tLoading sensor gps-speed ... done\n",
      "\t\tLoading sensor kinect-image ... done\n",
      "\t\tLoading sensor lidar ... done\n"
     ]
    }
   ],
   "source": [
    "dataframes = {}\n",
    "\n",
    "# All the dataframes will be stored classified by subject, dataset and sensor\n",
    "for subject in SUBJECTS:\n",
    "    print('Loading subject ' + subject)\n",
    "    subject_dfs = {}\n",
    "    # Each subject can have 1+ dataset types, so we store them sepparate\n",
    "    for dataset in DATASETS:\n",
    "        print('\\tLoading dataset ' + dataset)\n",
    "        dataset_dfs = {}\n",
    "        # The same with sensors. We store them inside its dataset's dictionary\n",
    "        for sensor in SENSORS:\n",
    "            print('\\t\\tLoading sensor ' + sensor + ' ... ', end='')\n",
    "            # Now we have all for locate the sensor. Let's get its name ...\n",
    "            filename = ds_filename(subject, dataset, sensor, 'csv')\n",
    "            # ... load it ...\n",
    "            df = pd.read_csv(os.path.join(paths.raw_csvs, filename))\n",
    "            # ... and store it in its dictionary\n",
    "            dataset_dfs[sensor] = df\n",
    "            print('done')\n",
    "        subject_dfs[dataset] = dataset_dfs\n",
    "    dataframes[subject] = subject_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will replace in each dataframe the `secs` and `nsecs` columns for a `time` column which will have the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T11:16:17.500568Z",
     "start_time": "2018-02-04T11:16:17.461536Z"
    }
   },
   "outputs": [],
   "source": [
    "for subject in SUBJECTS:\n",
    "    for dataset in DATASETS:\n",
    "        for sensor in SENSORS:\n",
    "            df = dataframes[subject][dataset][sensor]\n",
    "            df[TIME] = df['secs'] + df['nsecs'] * pow(10, -9)\n",
    "            df.drop(['secs', 'nsecs'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also change the name of the columns because they can be duped. For this, we'll append the set name to the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T11:16:18.049041Z",
     "start_time": "2018-02-04T11:16:18.014090Z"
    }
   },
   "outputs": [],
   "source": [
    "for subject in SUBJECTS:\n",
    "    for dataset in DATASETS:\n",
    "        for sensor in SENSORS:\n",
    "            df = dataframes[subject][dataset][sensor]\n",
    "            mapping = {\n",
    "                column: sensor + '_' + column\n",
    "                for column in df.columns\n",
    "            }\n",
    "            df.rename(columns=mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is synchronizing the dataset. For this purpose, the first step is get the rows of all the datasets that are nearest in time and adjust the starting index in each row to that position. We'll define the distance between times as its MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T11:16:19.300571Z",
     "start_time": "2018-02-04T11:16:19.209932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting dataset starting time for miguel\n",
      "\tNearest rows in subsets: 0, 2, 1, 42, 0\n",
      "\tNearest rows in subsets: 0, 4, 3, 113, 1\n"
     ]
    }
   ],
   "source": [
    "def starting_indices(dfs, columns):\n",
    "    def error(dfs, rows, cols):\n",
    "        return sum(\n",
    "            pow(dfs[i].loc[rows[i], cols[i]] - dfs[i + 1].loc[rows[i + 1], cols[i + 1]], 2)\n",
    "            for i in range(len(dfs) - 1)\n",
    "        )\n",
    "\n",
    "    # We start in 0 index for all the dataframes. This will be the best position (for now).\n",
    "    indices = [0 for _ in dfs]\n",
    "    min_error = error(dfs, indices, columns)\n",
    "    possible_indices = [(min_error, indices)]\n",
    "    while possible_indices:\n",
    "        del possible_indices[:]  # .clear() doesn't exists in python2\n",
    "        # We go one by one over all the dfs.\n",
    "        for i_df, df in enumerate(dfs):\n",
    "            # If there is a row over the current one, we check it's contents\n",
    "            if indices[i_df] < len(df.index):\n",
    "                new_indices = indices[:]\n",
    "                new_indices[i_df] += 1\n",
    "                # Is the new time difference better?\n",
    "                this_error = error(dfs, new_indices, columns)\n",
    "                if this_error <= min_error:\n",
    "                    possible_indices.append((this_error, new_indices))\n",
    "\n",
    "        # Si hay filas mejores que la actual, cogemos la mejor\n",
    "        if possible_indices:\n",
    "            possible_indices.sort()\n",
    "            min_error, indices = possible_indices[0]\n",
    "\n",
    "    return indices\n",
    "\n",
    "    # If it's necessary to remove columns, now it's the moment\n",
    "    if exclude_columns:\n",
    "        print(exclude_columns)\n",
    "        master_df = master_df[[col for col in master_df.columns if col not in exclude_columns]]\n",
    "\n",
    "for subject in SUBJECTS:\n",
    "    print('Adjusting dataset starting time for ' + subject)\n",
    "    for dataset in DATASETS:\n",
    "        sensors = dataframes[subject][dataset].keys()\n",
    "        dfs = [dataframes[subject][dataset][sensor] for sensor in sensors]\n",
    "        time_columns = [sensor + '_' + TIME for sensor in sensors]\n",
    "        # Get the indexes of the nearest rows\n",
    "        indices = starting_indices(dfs, time_columns)\n",
    "        print('\\tNearest rows in subsets: ' + ', '.join(map(str, indices)))\n",
    "        # Adjust the df to that starting indexes\n",
    "        for sensor, df, index in zip(sensors, dfs, indices):\n",
    "            dataframes[subject][dataset][sensor] = df.shift(-index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of clarity, after synchronizing all the dataframes, their timestamps will be set as relative to the smallest one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T11:16:20.639588Z",
     "start_time": "2018-02-04T11:16:20.623288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting dataset minimum time for miguel\n",
      "\tDataset: train\n",
      "\t\tMinimum time between sensors: 1517236091.42862\n",
      "\tDataset: test\n",
      "\t\tMinimum time between sensors: 1517237130.141652\n"
     ]
    }
   ],
   "source": [
    "for subject in SUBJECTS:\n",
    "    print('Adjusting dataset minimum time for ' + subject)\n",
    "    for dataset in DATASETS:\n",
    "        print('\\tDataset: ' + dataset)\n",
    "        dfs = [dataframes[subject][dataset][sensor] for sensor in SENSORS]\n",
    "        time_columns = [sensor + '_' + TIME for sensor in SENSORS]\n",
    "        minimum_value = min(df[tc].min() for df, tc in zip(dfs, time_columns))\n",
    "        print('\\t\\tMinimum time between sensors: ' + str(minimum_value))\n",
    "        for df, tc in zip(dfs, time_columns):\n",
    "            df[tc] -= minimum_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-04T11:19:19.669Z"
    }
   },
   "outputs": [],
   "source": [
    "def syncronize_dataframes(dfs, time_columns, freq=10, exclude_columns=None):\n",
    "    master_df = pd.DataFrame(columns=[col for df in dfs for col in df])\n",
    "    rows = [0 for _ in dfs]\n",
    "    step = 0\n",
    "    time = 1 / freq\n",
    "    half_time = time / 2\n",
    "    while all(row < len(df) - 1 for df, row in zip(dfs, rows)):\n",
    "        data_row = []\n",
    "        for df_i, (df, row, col) in enumerate(zip(dfs, rows, time_columns)):\n",
    "            possible_values = []\n",
    "            next_row = None\n",
    "            for i in range(len(df) - row):\n",
    "                value = df.loc[row + i, col]\n",
    "                diff = step * time - value\n",
    "                if -half_time < diff < half_time:\n",
    "                    # We're inside the thresshold so we take the value\n",
    "                    possible_values.append((value, row + i))\n",
    "                elif diff < -half_time:\n",
    "                    # We're over the thresshold, so no more values should be taken\n",
    "                    break\n",
    "\n",
    "            if possible_values:\n",
    "                possible_values.sort()\n",
    "                _, row = possible_values[0]\n",
    "                possible_values.clear()\n",
    "\n",
    "                data_row.extend(list(df.loc[row, :]))\n",
    "                rows[df_i] = row + 1\n",
    "            else:\n",
    "                data_row.extend([np.nan for _ in df.columns])\n",
    "\n",
    "        master_df.loc[step] = data_row\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    # If there are starting or ending rows with null data, we remove them too\n",
    "    while master_df[time_columns].loc[0, :].isnull().any():\n",
    "        master_df = master_df[1:]\n",
    "        master_df.reset_index(drop=True, inplace=True)\n",
    "    while master_df[time_columns].loc[len(master_df) - 1, :].isnull().any():\n",
    "        master_df = master_df[:-1]\n",
    "        master_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # If it's necessary to remove columns, now it's the moment\n",
    "    if exclude_columns:\n",
    "        master_df = master_df[[col for col in master_df.columns if col not in exclude_columns]]\n",
    "\n",
    "    return master_df\n",
    "\n",
    "master_datasets = {}\n",
    "for subject in SUBJECTS:\n",
    "    subject_dfs = {}\n",
    "    for dataset in DATASETS:\n",
    "        dfs = [dataframes[subject][dataset][sensor] for sensor in SENSORS]\n",
    "        tcs = [sensor + '_' + TIME for sensor in SENSORS]\n",
    "        freq = 10\n",
    "\n",
    "        subject_dfs[dataset] = syncronize_dataframes(dfs, tcs, freq=freq)\n",
    "    master_datasets[subject] = subject_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T22:31:58.520057Z",
     "start_time": "2018-02-03T22:31:57.680Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('float_format', '{:f}'.format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T22:31:58.521181Z",
     "start_time": "2018-02-03T22:31:57.682Z"
    }
   },
   "outputs": [],
   "source": [
    "a = ['a', 'b', 'c']\n",
    "a.replace('a', 'j')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
