{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T11:16:14.857283Z",
     "start_time": "2018-02-04T11:16:14.693172Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from settings import Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T11:16:15.085412Z",
     "start_time": "2018-02-04T11:16:15.076839Z"
    }
   },
   "outputs": [],
   "source": [
    "paths = Paths(\n",
    "    '/media/blazaid/Saca1/Phd/data',\n",
    "    'raw_csvs', 'synced_csvs', 'tmp'\n",
    ")\n",
    "\n",
    "TIME = 'time'  # The name of the time column which will be used for syncing\n",
    "SYNC_FREQ = 10  # hZ\n",
    "SUBJECTS = ['miguel']\n",
    "DATASETS = ['train', 'test']\n",
    "SENSORS = ['can', 'gps-position', 'gps-speed', 'kinect-image', 'lidar']\n",
    "\n",
    "def ds_filename(subject, dataset, sensor, extension):\n",
    "    return '-'.join([subject, dataset, sensor]) + '.' + extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the dataframes into memory to ease the work. They are not to heavy because the binary data (pointclouds and images) have been saved apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T11:16:16.673112Z",
     "start_time": "2018-02-04T11:16:16.103920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading subject miguel\n",
      "\tLoading dataset train\n",
      "\t\tLoading sensor can ... done\n",
      "\t\tLoading sensor gps-position ... done\n",
      "\t\tLoading sensor gps-speed ... done\n",
      "\t\tLoading sensor kinect-image ... done\n",
      "\t\tLoading sensor lidar ... done\n",
      "\tLoading dataset test\n",
      "\t\tLoading sensor can ... done\n",
      "\t\tLoading sensor gps-position ... done\n",
      "\t\tLoading sensor gps-speed ... done\n",
      "\t\tLoading sensor kinect-image ... done\n",
      "\t\tLoading sensor lidar ... done\n"
     ]
    }
   ],
   "source": [
    "dataframes = {}\n",
    "\n",
    "# All the dataframes will be stored classified by subject, dataset and sensor\n",
    "for subject in SUBJECTS:\n",
    "    print('Loading subject ' + subject)\n",
    "    subject_dfs = {}\n",
    "    # Each subject can have 1+ dataset types, so we store them sepparate\n",
    "    for dataset in DATASETS:\n",
    "        print('\\tLoading dataset ' + dataset)\n",
    "        dataset_dfs = {}\n",
    "        # The same with sensors. We store them inside its dataset's dictionary\n",
    "        for sensor in SENSORS:\n",
    "            print('\\t\\tLoading sensor ' + sensor + ' ... ', end='')\n",
    "            # Now we have all for locate the sensor. Let's get its name ...\n",
    "            filename = ds_filename(subject, dataset, sensor, 'csv')\n",
    "            # ... load it ...\n",
    "            df = pd.read_csv(os.path.join(paths.raw_csvs, filename))\n",
    "            # ... and store it in its dictionary\n",
    "            dataset_dfs[sensor] = df\n",
    "            print('done')\n",
    "        subject_dfs[dataset] = dataset_dfs\n",
    "    dataframes[subject] = subject_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will replace in each dataframe the `secs` and `nsecs` columns for a `time` column which will have the same information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T11:16:17.500568Z",
     "start_time": "2018-02-04T11:16:17.461536Z"
    }
   },
   "outputs": [],
   "source": [
    "for subject in SUBJECTS:\n",
    "    for dataset in DATASETS:\n",
    "        for sensor in SENSORS:\n",
    "            df = dataframes[subject][dataset][sensor]\n",
    "            df[TIME] = df['secs'] + df['nsecs'] * pow(10, -9)\n",
    "            df.drop(['secs', 'nsecs'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating CAN messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CAN messages are written as they come. We need to sepparate them into different columns, one for each message type, and then translate them to their real values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'frame', u'time', u'frame_type'], dtype='object')\n",
      "8FE1C003425002106E328\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def convert(msg, init_byte, n_bits, first_bit, resolution, offset):\n",
    "    endByte = int(init_byte - math.ceil((n_bits + first_bit) / 8.0) + 1)\n",
    "    totbits = (init_byte - endByte + 1) * 8\n",
    "    ss = msg[endByte * 2: (init_byte + 1) * 2]\n",
    "    s = int(ss, 16)\n",
    "    if n_bits > totbits or n_bits < 0:\n",
    "        return None\n",
    "    mask = '1' * (n_bits - 1)\n",
    "    #for j in range(n_bits - 1):\n",
    "    #    mask = mask << 1\n",
    "    #    mask = mask + 1\n",
    "    #mask = mask << first_bit\n",
    "    res = s & int(mask, 2)\n",
    "    res = res >> first_bit\n",
    "    return res*resolution+offset\n",
    "\n",
    "\n",
    "df = dataframes['miguel']['test']['can']\n",
    "print(df.columns)\n",
    "df['frame_type'] = df['frame'].str[1:4]\n",
    "for index, row in df.iterrows():\n",
    "    if row['frame_type'] == '412':\n",
    "        print(row['frame'][4:])\n",
    "        print(convert(row['frame'][4:], 1, 9, 0, 1, 0))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n",
      "374\n",
      "208\n",
      "424\n",
      "346\n",
      "236\n",
      "231\n",
      "696\n",
      "2F2\n",
      "412\n"
     ]
    }
   ],
   "source": [
    "frames_dfs = []\n",
    "for frame_type in  df['frame_type'].unique():\n",
    "    print(frame_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['418', '374', '208', '424', '346', '236', '231', '696', '2F2',\n",
       "       '412'], dtype=object)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronizing all sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will change the name of the columns because they can be duped. For this, we'll append the set name to the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T11:16:18.049041Z",
     "start_time": "2018-02-04T11:16:18.014090Z"
    }
   },
   "outputs": [],
   "source": [
    "for subject in SUBJECTS:\n",
    "    for dataset in DATASETS:\n",
    "        for sensor in SENSORS:\n",
    "            df = dataframes[subject][dataset][sensor]\n",
    "            mapping = {\n",
    "                column: sensor + '_' + column\n",
    "                for column in df.columns\n",
    "            }\n",
    "            df.rename(columns=mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is synchronizing the dataset. For this purpose, the first step is get the rows of all the datasets that are nearest in time and adjust the starting index in each row to that position. We'll define the distance between times as its MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T11:16:19.300571Z",
     "start_time": "2018-02-04T11:16:19.209932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting dataset starting time for miguel\n",
      "\tNearest rows in subsets: 0, 2, 1, 42, 0\n",
      "\tNearest rows in subsets: 0, 4, 3, 113, 1\n"
     ]
    }
   ],
   "source": [
    "def starting_indices(dfs, columns):\n",
    "    def error(dfs, rows, cols):\n",
    "        return sum(\n",
    "            pow(dfs[i].loc[rows[i], cols[i]] - dfs[i + 1].loc[rows[i + 1], cols[i + 1]], 2)\n",
    "            for i in range(len(dfs) - 1)\n",
    "        )\n",
    "\n",
    "    # We start in 0 index for all the dataframes. This will be the best position (for now).\n",
    "    indices = [0 for _ in dfs]\n",
    "    min_error = error(dfs, indices, columns)\n",
    "    possible_indices = [(min_error, indices)]\n",
    "    while possible_indices:\n",
    "        del possible_indices[:]  # .clear() doesn't exists in python2\n",
    "        # We go one by one over all the dfs.\n",
    "        for i_df, df in enumerate(dfs):\n",
    "            # If there is a row over the current one, we check it's contents\n",
    "            if indices[i_df] < len(df.index):\n",
    "                new_indices = indices[:]\n",
    "                new_indices[i_df] += 1\n",
    "                # Is the new time difference better?\n",
    "                this_error = error(dfs, new_indices, columns)\n",
    "                if this_error <= min_error:\n",
    "                    possible_indices.append((this_error, new_indices))\n",
    "\n",
    "        # Si hay filas mejores que la actual, cogemos la mejor\n",
    "        if possible_indices:\n",
    "            possible_indices.sort()\n",
    "            min_error, indices = possible_indices[0]\n",
    "\n",
    "    return indices\n",
    "\n",
    "    # If it's necessary to remove columns, now it's the moment\n",
    "    if exclude_columns:\n",
    "        print(exclude_columns)\n",
    "        master_df = master_df[[col for col in master_df.columns if col not in exclude_columns]]\n",
    "\n",
    "for subject in SUBJECTS:\n",
    "    print('Adjusting dataset starting time for ' + subject)\n",
    "    for dataset in DATASETS:\n",
    "        sensors = dataframes[subject][dataset].keys()\n",
    "        dfs = [dataframes[subject][dataset][sensor] for sensor in sensors]\n",
    "        time_columns = [sensor + '_' + TIME for sensor in sensors]\n",
    "        # Get the indexes of the nearest rows\n",
    "        indices = starting_indices(dfs, time_columns)\n",
    "        print('\\tNearest rows in subsets: ' + ', '.join(map(str, indices)))\n",
    "        # Adjust the df to that starting indexes\n",
    "        for sensor, df, index in zip(sensors, dfs, indices):\n",
    "            dataframes[subject][dataset][sensor] = df.shift(-index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of clarity, after synchronizing all the dataframes, their timestamps will be set as relative to the smallest one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-04T11:16:20.639588Z",
     "start_time": "2018-02-04T11:16:20.623288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting dataset minimum time for miguel\n",
      "\tDataset: train\n",
      "\t\tMinimum time between sensors: 1517236091.42862\n",
      "\tDataset: test\n",
      "\t\tMinimum time between sensors: 1517237130.141652\n"
     ]
    }
   ],
   "source": [
    "for subject in SUBJECTS:\n",
    "    print('Adjusting dataset minimum time for ' + subject)\n",
    "    for dataset in DATASETS:\n",
    "        print('\\tDataset: ' + dataset)\n",
    "        dfs = [dataframes[subject][dataset][sensor] for sensor in SENSORS]\n",
    "        time_columns = [sensor + '_' + TIME for sensor in SENSORS]\n",
    "        minimum_value = min(df[tc].min() for df, tc in zip(dfs, time_columns))\n",
    "        print('\\t\\tMinimum time between sensors: ' + str(minimum_value))\n",
    "        for df, tc in zip(dfs, time_columns):\n",
    "            df[tc] -= minimum_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-04T11:19:19.669Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8656f27fb6ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mfreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0msubject_dfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msyncronize_dataframes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mmaster_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubject_dfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-8656f27fb6ab>\u001b[0m in \u001b[0;36msyncronize_dataframes\u001b[0;34m(dfs, time_columns, freq, exclude_columns)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mdata_row\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mmaster_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_setitem_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_with_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_has_valid_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36m_setitem_with_indexer\u001b[0;34m(self, indexer, value)\u001b[0m\n\u001b[1;32m    443\u001b[0m                                        name=indexer)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_update_cacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mappend\u001b[0;34m(self, other, ignore_index, verify_integrity)\u001b[0m\n\u001b[1;32m   5192\u001b[0m             \u001b[0mto_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5193\u001b[0m         return concat(to_concat, ignore_index=ignore_index,\n\u001b[0;32m-> 5194\u001b[0;31m                       verify_integrity=verify_integrity)\n\u001b[0m\u001b[1;32m   5195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5196\u001b[0m     def join(self, other, on=None, how='left', lsuffix='', rsuffix='',\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/reshape/concat.pyc\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    211\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                        copy=copy)\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/reshape/concat.pyc\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    406\u001b[0m             new_data = concatenate_block_managers(\n\u001b[1;32m    407\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                 copy=self.copy)\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   5201\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5202\u001b[0m             b = make_block(\n\u001b[0;32m-> 5203\u001b[0;31m                 \u001b[0mconcatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5204\u001b[0m                 placement=placement)\n\u001b[1;32m   5205\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m   5326\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Concatenating join units along axis0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5328\u001b[0;31m     \u001b[0mempty_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupcasted_na\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_empty_dtype_and_na\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5330\u001b[0m     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget_empty_dtype_and_na\u001b[0;34m(join_units)\u001b[0m\n\u001b[1;32m   5279\u001b[0m         \u001b[0;31m# are only null blocks, when same upcasting rules must be applied to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5280\u001b[0m         \u001b[0;31m# null upcast classes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5281\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_na\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5282\u001b[0m             \u001b[0mnull_upcast_classes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupcast_cls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5283\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.cache_readonly.__get__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mis_na\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5572\u001b[0m         \u001b[0mchunk_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_len\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5573\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5574\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues_flat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchunk_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5575\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/dtypes/missing.pyc\u001b[0m in \u001b[0;36misna\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0malias\u001b[0m \u001b[0mof\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \"\"\"\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_isna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/dtypes/missing.pyc\u001b[0m in \u001b[0;36m_isna_new\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"isna is not defined for MultiIndex\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mABCSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_isna_ndarraylike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCGeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/core/dtypes/missing.pyc\u001b[0m in \u001b[0;36m_isna_ndarraylike\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnaobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def syncronize_dataframes(dfs, time_columns, freq=10, exclude_columns=None):\n",
    "    master_df = pd.DataFrame(columns=[col for df in dfs for col in df])\n",
    "    rows = [0 for _ in dfs]\n",
    "    step = 0\n",
    "    time = 1 / freq\n",
    "    half_time = time / 2\n",
    "    while all(row < len(df) - 1 for df, row in zip(dfs, rows)):\n",
    "        data_row = []\n",
    "        for df_i, (df, row, col) in enumerate(zip(dfs, rows, time_columns)):\n",
    "            possible_values = []\n",
    "            next_row = None\n",
    "            for i in range(len(df) - row):\n",
    "                value = df.loc[row + i, col]\n",
    "                diff = step * time - value\n",
    "                if -half_time < diff < half_time:\n",
    "                    # We're inside the thresshold so we take the value\n",
    "                    possible_values.append((value, row + i))\n",
    "                elif diff < -half_time:\n",
    "                    # We're over the thresshold, so no more values should be taken\n",
    "                    break\n",
    "\n",
    "            if possible_values:\n",
    "                possible_values.sort()\n",
    "                _, row = possible_values[0]\n",
    "                possible_values.clear()\n",
    "\n",
    "                data_row.extend(list(df.loc[row, :]))\n",
    "                rows[df_i] = row + 1\n",
    "            else:\n",
    "                data_row.extend([np.nan for _ in df.columns])\n",
    "\n",
    "        master_df.loc[step] = data_row\n",
    "\n",
    "        step += 1\n",
    "\n",
    "    # If there are starting or ending rows with null data, we remove them too\n",
    "    while master_df[time_columns].loc[0, :].isnull().any():\n",
    "        master_df = master_df[1:]\n",
    "        master_df.reset_index(drop=True, inplace=True)\n",
    "    while master_df[time_columns].loc[len(master_df) - 1, :].isnull().any():\n",
    "        master_df = master_df[:-1]\n",
    "        master_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # If it's necessary to remove columns, now it's the moment\n",
    "    if exclude_columns:\n",
    "        master_df = master_df[[col for col in master_df.columns if col not in exclude_columns]]\n",
    "\n",
    "    return master_df\n",
    "\n",
    "master_datasets = {}\n",
    "for subject in SUBJECTS:\n",
    "    print('Synchronizing dataframes for ' + subject)\n",
    "    subject_dfs = {}\n",
    "    for dataset in DATASETS:\n",
    "        print('\\tSynchronizing {} at {} hz'.format(dataset, SYNC_FREQ))\n",
    "        dfs = [dataframes[subject][dataset][sensor] for sensor in SENSORS]\n",
    "        tcs = [sensor + '_' + TIME for sensor in SENSORS]\n",
    "        freq = SYNC_FREQ\n",
    "        print('\\tDataframe created')\n",
    "        subject_dfs[dataset] = syncronize_dataframes(dfs, tcs, freq=freq)\n",
    "    master_datasets[subject] = subject_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T22:31:58.520057Z",
     "start_time": "2018-02-03T22:31:57.680Z"
    }
   },
   "outputs": [],
   "source": [
    "master_datasets[SUBJECTS[0]][DATASETS[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-03T22:31:58.521181Z",
     "start_time": "2018-02-03T22:31:57.682Z"
    }
   },
   "outputs": [],
   "source": [
    "# Traducir la columna de "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
