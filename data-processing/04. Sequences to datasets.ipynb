{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import os, glob\n",
    "import shutil\n",
    "from pprint import pprint\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pynsia.pointcloud import Deepmap\n",
    "\n",
    "figsize = (8, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIG_PATH = '/media/blazaid/Saca/Phd/data/curated'\n",
    "DEST_PATH = '/media/blazaid/Saca/Phd/data/datasets'\n",
    "DMS_DIR = 'deepmaps'\n",
    "SUBJECTS = 'edgar', 'jj', 'miguel'\n",
    "MOMENTS_BEFORE = [5, 10, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DM_DIR = os.path.join(ORIG_PATH, 'deepmaps')\n",
    "CF = 'cf'\n",
    "LC = 'lc'\n",
    "MOMENTS_BEFORE.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sequences(path, subjects):\n",
    "    sequences = {}\n",
    "    for dataset in ('training', 'validation'):\n",
    "        print('{} dataset'.format(dataset))\n",
    "        sequences[dataset] = {}\n",
    "        for submodel in (CF, LC):\n",
    "            print('\\t{} model'.format(submodel))\n",
    "            sequences[dataset][submodel] = {}\n",
    "            for subject in subjects:\n",
    "                print('\\t\\tLoading data for subject {} .. '.format(subject), end='')\n",
    "                file_pattern = os.path.join(path, '{}_{}_{}_*.csv'.format(submodel, subject, dataset))\n",
    "                sequences[dataset][submodel][subject] = [\n",
    "                    pd.read_csv(filepath, index_col=None, engine='python')\n",
    "                    for filepath in glob.glob(file_pattern)\n",
    "                ]\n",
    "                print('{} loaded'.format(len(sequences[dataset][submodel][subject])))\n",
    "\n",
    "    for dataset in ('training', 'validation'):\n",
    "        for submodel in (LC, CF):\n",
    "            sequences[dataset][submodel]['all'] = []\n",
    "            for subject in subjects:\n",
    "                sequences[dataset][submodel]['all'].extend(sequences[dataset][submodel][subject])\n",
    "\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset\n",
      "\tcf model\n",
      "\t\tLoading data for subject edgar .. 10 loaded\n",
      "\t\tLoading data for subject jj .. 6 loaded\n",
      "\t\tLoading data for subject miguel .. 12 loaded\n",
      "\tlc model\n",
      "\t\tLoading data for subject edgar .. 1584 loaded\n",
      "\t\tLoading data for subject jj .. 1958 loaded\n",
      "\t\tLoading data for subject miguel .. 1914 loaded\n",
      "validation dataset\n",
      "\tcf model\n",
      "\t\tLoading data for subject edgar .. 3 loaded\n",
      "\t\tLoading data for subject jj .. 5 loaded\n",
      "\t\tLoading data for subject miguel .. 7 loaded\n",
      "\tlc model\n",
      "\t\tLoading data for subject edgar .. 484 loaded\n",
      "\t\tLoading data for subject jj .. 660 loaded\n",
      "\t\tLoading data for subject miguel .. 660 loaded\n"
     ]
    }
   ],
   "source": [
    "base_sequences = load_sequences(ORIG_PATH, SUBJECTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(DEST_PATH):\n",
    "    os.makedirs(DEST_PATH)\n",
    "if not os.path.isdir(os.path.join(DEST_PATH, DMS_DIR)):\n",
    "    os.makedirs(os.path.join(DEST_PATH, DMS_DIR))\n",
    "\n",
    "for filename in glob.glob(os.path.join(DEST_PATH, '*')):\n",
    "    if not os.path.isdir(filename):\n",
    "        os.remove(filename)\n",
    "for filename in glob.glob(os.path.join(DEST_PATH, DMS_DIR, '*')):\n",
    "    os.remove(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building datasets\n",
      "\tBuilding cf training dataset for moments all ...done\n",
      "\tSaving dataset cf-all-training-t-t5-t10-t20.csv ... done\n",
      "Building datasets\n",
      "\tBuilding cf training dataset for moments miguel ...done\n",
      "\tSaving dataset cf-miguel-training-t-t5-t10-t20.csv ... done\n",
      "Building datasets\n",
      "\tBuilding cf training dataset for moments jj ...done\n",
      "\tSaving dataset cf-jj-training-t-t5-t10-t20.csv ... done\n",
      "Building datasets\n",
      "\tBuilding cf training dataset for moments edgar ...done\n",
      "\tSaving dataset cf-edgar-training-t-t5-t10-t20.csv ... done\n",
      "Building datasets\n",
      "\tBuilding lc training dataset for moments all ...done\n",
      "\tSaving dataset lc-all-training-t-t5-t10-t20.csv ... done\n",
      "\tSaving deepmaps ... "
     ]
    }
   ],
   "source": [
    "for dataset in base_sequences:\n",
    "    for submodel in base_sequences[dataset]:\n",
    "        for subject in base_sequences[dataset][submodel]:\n",
    "            print('Building datasets')\n",
    "            dfs = base_sequences[dataset][submodel][subject]\n",
    "            # And now, construct the dataset\n",
    "            print('\\tBuilding {} {} dataset for moments {} ...'.format(submodel, dataset, subject), end='')\n",
    "            moments_suffix = 't-' + '-'.join([] + ['t{}'.format(x) for x in MOMENTS_BEFORE])\n",
    "            filename = '{}-{}-{}-{}.csv'.format(submodel, subject, dataset, moments_suffix)\n",
    "            \n",
    "            if submodel == CF:\n",
    "                datasets = pd.concat(dfs, ignore_index=True) \n",
    "            else:\n",
    "                datasets = []\n",
    "                temporal_columns = ['Acceleration', 'Next TLS status', 'Deepmap', 'Relative speed']\n",
    "                for df in dfs:\n",
    "                    # Generate the dataframes with the shifted times\n",
    "                    subset = df\n",
    "                    for moment in MOMENTS_BEFORE:\n",
    "                        temp_df = df.shift(moment)\n",
    "                        \n",
    "                        suffix = ' t_{}'.format(moment)\n",
    "                        for column in temporal_columns:\n",
    "                            subset[column + suffix] = temp_df[column]\n",
    "                    subset = subset[max(MOMENTS_BEFORE):]\n",
    "                    datasets.append(subset)\n",
    "                \n",
    "                datasets = pd.concat(datasets, ignore_index=True)\n",
    "            \n",
    "            print('done')\n",
    "            print('\\tSaving dataset {} ... '.format(filename), end='')\n",
    "            datasets.to_csv(os.path.join(DEST_PATH, filename), index=False)\n",
    "            print('done')\n",
    "            if submodel == LC:\n",
    "                print('\\tSaving deepmaps ... ', end='')\n",
    "                dm_columns = [c for c in datasets.columns if c.startswith('Deepmap')]\n",
    "                for index, row in datasets.iterrows():\n",
    "                    for column in dm_columns:\n",
    "                        if not os.path.exists(os.path.join(DEST_PATH, row[column])):\n",
    "                            shutil.copy(\n",
    "                                os.path.join(ORIG_PATH, row[column]),\n",
    "                                os.path.join(DEST_PATH, DMS_DIR),\n",
    "                            )\n",
    "                print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
