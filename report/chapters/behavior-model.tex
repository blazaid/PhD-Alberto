\chapter{Modelo de comportamiento de conductor}
\label{ch:behavior-model}

\section{Introducción}

\TODO{no sé qué poner}

\TODO{Hablar del esquema general (a muy grandes rasgos) del modelo de conducción que se pretende conseguir. Por qué es así, por qué queremos esas salidas y qué limitaciones nos impone el simulador sobre el que trabajamos. Aunque SUMO es continuo en el espacio, vamos sobre raíles.}

El modelo será entrenado siguiendo un esquema supervisado, por lo que necesitamos datos reales a partir de los cuales deberá ajustar su funcionamiento.

La información que se considera suficiente para que los modelos desempeñen su función en el entorno de simulación y la razón por la cual se ha tenido en cuenta es la siguiente:

\begin{itemize}
	\item \textbf{Entorno} El conductor, y por tanto el modelo, desarrolla su actividad y basa sus comportamientos, entre otros factores, en el entorno en el que se encuentra inmerso. Se considera por tanto que el ajuste de la velocidad y, sobre todo los cambios de carril, se ven influenciados por éste.
	\item \textbf{Velocidad actual del vehículo y velocidad máxima del carril}. Tanto la velocidad en la vía como del vehículo en un momento concreto influye en cómo el conductor va a modificar su aceleración en momentos posteriores.
	\item \textbf{Distancia y diferencia de velocidad con el vehículo delantero}. Al igual que con la velocidad, el vehículo delantero juega un papel esencial en los cambios de aceleración. También se intuye que puede influir en el comportamiento de cambio de carril en casos en los que el vehículo delantero va muy lento
	\item \textbf{Siguiente salida y carriles cortados}. Se considera que este tipo de información es crucial a la hora de realizar cambios de carril, ya que además de lo obvio, puede influir en otras maniobras tales como un adelantamiento.
	\item \textbf{Señales}. Es interesante contar con este tipo de señales ya que pueden influir en diferentes patrones de aceleración (e.g. estamos cerca y cambia de color a ámbar) o desaceleración (e.g. aproximación a un ceda el paso, stop o semáforo en rojo).
\end{itemize}

\section{Extracción de datos de conducción}

Se ha hecho uso de un vehículo instrumentado con sensores para capturar la mayor cantidad posible de información especificada en la introducción. El vehículo en cuestión es un Mitsubishi iMiEV (Figura~\ref{fig:instrumented-imiev}) y los dispositivos un LIDAR, un GPS, el Bus CAN y una cámara.

\begin{figure}
	\includegraphics{instrumented-imiev}
	\caption{El vehículo utilizado en los ensayos realizados. Se trata de un Mitsubishi iMiEV instrumentado con un LIDAR anclado en la baca superior, un GPS anclado en el techo, un puerto de acceso directo al Bus CAN y una cámara Microsoft Kinect tras el espejo retrovisor.}
	\label{fig:instrumented-imiev}
\end{figure}

Todos los dispositivos se conectan a un ordenador con sistema operativo GNU/Linux sobre Intel i7-7500U CPU con 16GB de memoria RAM siguiendo el esquema que se muestra en la Figura~\ref{fig:instrumented-imiev-schema}. Al ser éstos dispositivos con capacidades diferentes, se ha optaco por la creación de una aplicación basada en el framework ROS del cual se da una breve visión general en el apéndice~\ref{ch:ros-overview}. A continuación se describen los dispositivos usados y su información generada.

\paragraph{LIDAR}

\TODO{Descripción detallada de qué es el lidar. Marca y cositas. Algo del estilo mide la distancia a través la diferencia entre la emisión de pulsos de luz y su reflejo en un sensor. A lo mejor queda bien en un sidenote. El usado es un Velodyne VLP-16, un LiDAR circular de 16 canales verticales separados 2º, dando un FOV (-15, 15) grados de apertura vertical). Este dispositivo se conecta a la máquina a través del puerto ethernet y se usa para la obtención de información del entorno del conductor. El LiDAR está situado en el techo del vehículo orientando los (0, 0) grados en al dirección y sentido de éste y con el plano horizontal paralelo al suelo.}

\TODO{Descripción de los datos que se extraen y cómo se almacenan.}

\paragraph{GPS}

\TODO{Yo creo que se podría decir que se usa para obtener una segunda medida de velocidad y aceleración, así como para la localización espacial de subconjuntos de datos interesantes para su estudio.}

\TODO{Descripción de los datos que se extraen y cómo se almacenan. Indicar los dos tipos de mensaje que se obtienen!}

\TODO{Descripción a través de qué puerto se conectan.}

\paragraph{Bus CAN}

\TODO{Explicar qué es el CAN Bus, y si es necesario, un poquitín su funcionamiento (aunque sea en un sidenote, así queda todo más completito.). El BUS del vehículo se conecta a través del puerto USB al ordenador a lo mejor hay que explicar un poco más la conexión, el tipo de cable y tal. Es usado para la extracción de la interacción del conductor con el vehículo.}

\TODO{Descripción de los datos que se extraen y cómo se almacenan.}

\TODO{Descripción a través de qué puerto se conectan.}

\TODO{Indicar que se ha desarrollado un NODO propio para la captura y que se encuentra incluido en el apéndice de ``software desarrollado''}

\paragraph{Cámara}

\TODO{Descripción de la cámara. Es el Kinect, pero hay que pillar versión, capacidades y demás. A lo mejor alguna ilustración con el rviz puede estar bien}

\TODO{Descripción de los datos que se extraen y cómo se almacenan.}

\TODO{Descripción a través de qué puerto se conectan.}

\subsection{Selección de rutas}

Para la captura de datos se han propuesto dos rutas, en adelante $R_1$ y $R_2$, consideradas equivalentes. Se tratan de vías en entorno urbano con tramos de entre uno y tres carriles a lo largo de su recorrido y con velocidades máximas de vía que oscilan entre los \SI{30}{\km\per\hour} y los \SI{50}{\km\per\hour}. La figura~\ref{fig:the-two-routes}

\begin{figure}
	\centering
	\subfloat[]{\missingfigure[figwidth=.43\textwidth]{Recorrido número 1 (el de la albufera)}}\qquad	\subfloat[]{\missingfigure[figwidth=.43\textwidth]{Recorrido número 2 (el del pueblo de vallecas)}}
	\caption[Dos recorridos para la captura de datos de conducción]{Los dos recorridos realizados para la captura de datos de conducción, ambos en entorno urbano. (a) $R_1$ tiene una duración estimada de \SI{30}{\minute} y sirve para la captura de los datos de entrenamiento. (b) $R_2$ tiene una duración estimada de \SI{15}{\minute} y sus datos serán utilizdos para los conjuntos de test.}
	\label{fig:the-two-routes}
\end{figure}

$R_1$ tiene una duración de recorrido estimada de \SI{30}{\minute} y se utilizará como fuente de datos destinada al entrenamiento del modelo (conjuntos de entrenamiento y de validación). $R_2$ por su lado tiene un tiempo estimado de recorrido de  \SI{15}{\minute} y sus datos tienen el propósito de servir de conjunto de test.

\subsection{Selección de sujetos}

Se han elegido un total de tres sujetos para los experimentos. Los tres pertenencen al grupo especificado en los supuestos del capítulo~\nameref{ch:intro}, es decir varones dentro del rango de $35$ a $39$ años.

Los sujetos tienen experiencia de conducción y han realizado el recorrido anteriormente a fin de basar sus comportamientos lo más posible al nivel táctico de conducción\sidenote{
	Dado que los comportamientos de \textit{car-following} y \textit{lane-change} se asocian con el nivel cognitivo táctico, se ha querido reducir el impacto del operador decidiendo durante el experimento hacia dónde o no ir. De esta manera los conductores son librese de realizar el movimiento que deseen y de anticipar maniobras con más libertad.
}.

\TODO{Quizá debería decir algo más?}

\section{Preparación de los datos}

Tras la captura se ha realizado una secuencia de pasos para dejar los datos preparados para el proceso de entrenamiento de los modelos. El resto de la sección detalla cada uno de éstos.

\subsection{Fusión de sensores}

Como hemos visto anteriormente, cada uno de los dispositivos ofrece sus datos a una tasa de frecuencia diferente (con excepción del bus CAN, en el cual los datos van a frecuencias diferentes).

El primer paso en la preparación de los datos ha sido el de la fusión de estos. Afortunadamente cada uno de los mensajes almacenados que llegan desde nodos de ROS vienen con una marca temporal que podemos suponer sincronizada entre nodos de la misma aplicación. Por ello, la fusión se ha realizado en dos pasos:

\begin{enumerate}
	\item Cálculo del primer elemento a fusionar de cada uno de los conjuntos de datos. Para ello, se han desechado todos los primeros valores hasta encontrar la primera tupla de valores (uno por cada conjunto de datos) donde éstos están más próximos entre si.
	\item Extracción iterativa de las tuplas más aproximadas. Iterativamente, se han ido extrayendo las tuplas más próximas a cada uno de los incrementos de la tasa deseada de sincronización $f$, siempre y cuando se encuentren dentro del intervalo $f \pm \frac{f}{2}$.
\end{enumerate}

Este proceso se ha repetido con cada uno de los conductores para cada uno de los recorridos, dando como resultado seis conjuntos sincronizados con los datos en bruto sincronizados.

\subsection{Extracción de variables no observables directamente}

Existen una serie de variables cuya extracción directa del entorno no es trivial. Ésta es una de las razones por las que se ha capturado con la cámara la visión del conductor en el recorrido.

El proceso de obtenición ha sido manual, obteniendo las marcas temporales de las variables a capturar tras el visionado de las imágenes capturadas por la cámara. Estas variables son las siguientes:

\paragraph{Cambio de carril}

Se han identificado los cambios de carril, siendo estos marcados como $+1$ si es un cambio hacia la izquierda o $-1$ si es un cambio a la derecha, y por tanto, todos aquellos momentos en los que no hay cambio de carril se marcan tendrán un valor de $0$. Las marcas temporales de los cambios son aquellas desde el comienzo de la maniobra del cambio de carril hasta que el vehículo ha llegado a la mitad del cambio.

\paragraph{Velocidad máxima de la vía}

Se han añadido las velocidades máximas de las vías indicando las marcas temporales en las que el conductor entra o sale de cada uno de los tramos.

\paragraph{Distancia y estado de semáforos}

La distancia al semáforo ha sido obtenida a partir de la distancia euclídea entre la geoposición del origen de coordenadas y la geoposición del semáforo, por lo que es esperable cierto márgen de error. Los estados se han extraído directamente del visionado de las imágenes, tomando este los valores $g$, $y$ y $r$ dependiendo de si el semáforo se encuentra en verde, ámbar o rojo.

\paragraph{Distancia a recorrer en carriles}

Al igual que con la distancia a los semáforos, ésta se ha calculado a partir de la distancia euclídea de la geoposición del origen de coordenadas a los puntos a partir de los cuales no se puede continuar por el recorrido especificado.

Las distancias obtenidas se corresponden al carril izquiero, al actual y al derecho.

\paragraph{Distancia al obstáculo más cercano}

Para el cálculo de esta variable, se ha procedido a capturar una región de interés de cada una de las nubes de puntos dentro de la cual identificar los posibles obstáculos existentes. Por la posición del lidar se ha decidido que ésta está acotada entre los intervalos $(0.35, 35)$, $(-1, 1)$ y $(-1.5, 0.5)$ para los ejex X, Y y Z respectivamente.

Posteriormente, para la nube de puntos resultante se ha realizado un proceso de clusterización aplicando el algoritmo DBSCAN\sidenote{
	DBSCAN~\cite{ester1996density} es un algoritmo de clusterización que identifica un número variable de conjuntos en un espació $n$-dimensional.
	
	Funciona a partir de la agregación de puntos en función de sus parámetros $\epsilon$ y $\mu$. $\epsilon$ es la distancia mínima a la que se deben encontrar dos puntos para considerarse pertenecientes al mismo clúster mientras que $\mu$ determina el número mínimo de puntos que debe tener un clúster para ser considerado como tal.
} con parámetros $\epsilon = 0.5$ y $\mu = 3$. Este proceso identifica un número variable de clústers, tras el cual nos hemos quedado con el más cercando al vehículo.

Posteriormente y de forma manual, se ha realizado el recorrido mostrando las nubes de puntos correspondientes a las capturas superponiendo el centroide para eliminar los de aquellos frames que se corresponden con errores. De los restantes se ha calculado una distancia euclídea al origen de coordenadas.

\subsection{Curación de datos}

Tras obtener todas las variables principales, ya sean obtenidas directamente de los sensores del coche o a través de un proceso manual vamos a generar los conjuntos de datos para los comportamientos longitudinal y de cambio de carril. La tabla~\ref{tbl:main-variables} describe qué variables son usadas en qué conjunto de datos.

\begin{table}[t]
	\caption[Resúmen de información extraída del vehículo instrumentado][1.5em]{Valores capturados por el vehículo instrumentado y sus dominios. Los valores de 0, 1 y 2 se corresponden con los cambios de carril, siendo 0 cambio a la izquierda, 1 no cambio y 2 a la derecha.}
	\label{tbl:main-variables}
	\begin{tabular}{lll}
		\toprule
		Variable & Longitudinal & Cambio de carril \\
		\midrule
		Acceleration      & \yep & \yep \\
		Distance +1       & \nop & \yep \\
		Distance 0        & \nop & \yep \\
		Distance -1       & \nop & \yep \\
		Lane change       & \nop & \yep \\
		Leader distance   & \yep & \nop \\
		Next TLS distance & \yep & \yep \\
		Next TLS status   & \yep & \yep \\
		Nube de puntos    & \nop & \yep \\
		Relative speed    & \yep & \yep \\
		Speed to leader   & \yep & \nop \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Data augmentation}

Una de las características del nuestro conjunto de datos para el modelo de cambio decarril es la nube de puntos. Los problemas con alta variabilidad de sus entradas suelen ser complejos y requerir de conjuntos de datos de tamaños bastante grandes para poder identificar patrones, como lo son por ejemplo los problemas de reconocimiento de imágenes.

En nuestro caso, el modelo de cambio de carril tiene como entrada una nube de puntos, la cual representa en un espacio de 3 dimensiones un conjunto muy limitado de puntos, con la dificultad añadida de que el LIDAR tiene de base un error de \SI{3}{\cm}. Como el espacio sobre el que trabajar es tan complejo, se requerirían modelos con muchos parámetros, pero al disponer de pocos ejemplos, podríamos caer muy fácilmente en problemas de \textit{over-fitting}.

Por ello, se ha optado por realizar un proceso de generación de datos artificiales a partir de los datos existentes. De esta manera, ayudaremos al modelo a entrenar con casos similares y así ayudarla a que generalice mejor. Concretamente hemos hecho uso de dos técnicas, \textit{mirroring} y \textit{shaking}.

En el proceso de mirroring, para cada fila generamos una nueva nube de puntos a partir de una simetría respecto al plano YZ
In the process of mirroring, we assume that the cognitive mechanisms that drive the lane change execution towards one side in a situation are the same that drives the execution to the opposite side in a mirrored situation (with respect to the YZ plane). An example of this concept is illustrated in Figure 7.


(a)
(b)
Figure 7. An example of the mirroring technique: (a) A regular frame captured from the LIDAR; (b) The same frame after the mirroring process.
By augmenting data with mirroring, the number of examples is doubled with the advantage of not losing any precision in the data.
In the technique we have called shaking, we take advantage of the imprecise nature of the LIDAR data in three-dimensional space. Considering an accuracy of ±3cm, we have a sphere of imprecision of 3cm of radius for each point in which we can situate it randomly. The name shaking refers as if we shake all the points of a frame (Figure 8).
The process is as follows: for each sequence we get all the frames and, for each of them, we go through each point and apply it a new random shift of (∆x, ∆y, ∆z). After this process, a new sequence is created with the same lane-change intention and action, and similar but slightly different environment to the original one.


(a)
(b)
Figure 8. An example of two frames after a shaking process over the frame displayed in Figure 7: (a) Shaking of ∆x=∆y=∆z=0.03; (b) Shaking of ∆x=∆y=∆z=0.5.
The intuition that this solution can help reducing the variance of the problem is that we assume that high precision is not required and that, therefore, two similar point clouds will produce similar effects. Due to the LIDAR imprecision, we can play with all the 3cm spaces around each point without, in theory, lose any precision.
In our case, new sequences are generated for each original and mirrored frame. For each point of the frame, its position is shifted (∆x, ∆y, ∆z) being ∆x, ∆y and ∆z a uniform value belonging to the open interval (-0.03, 0.03, 0.03) ⊂ R or (-0.05, 0.05, 0.05) ⊂ R, depending on the stage of the training process.
4.4.2. Data representation
These sequences contain, apart from the intention and execution of lane changes of the driver, the surroundings of the vehicle as a point cloud. The problem here lies in the requirement of the ANNs that need the input as a fixed set of elements, and the number of points in a point cloud varies depending on the number of elements in the surrounding where the laser impacts. For this solution a representation as a 2D deepness map is used.
In this sense, the data acquired by our laser scanner is transformed into the image domain. Thus, the distance information of each point is projected into a 2D matrix where the vertical axis corresponds to the elevation angles and the horizontal axis is the azimuth angles. The former is discretized between −7 and 3 degrees with a step of 2 degrees; values lower than -7 degrees lead to collisions with the ego car, whilst the ones higher than 3 degrees tend to collide with obstacles very far away, thus they’re not considered relevant. The latter values (azimuth) corresponds to a 360 degrees wide field of view, discretized with a step of 1 degree. This configuration allows the transformation of all the point clouds into images with the same resolution of 6 × 360 pixels (Figure 9).

One problem that occurs using LIDAR sensors is that the divergence between points grows with the distance to them. Due this divergence in the data, the interval for the vertical field of view has been selected considering the sparse data acquired with high values of the elevation angles.
All the values in the deepness map are normalized to [0, 1]. Therefore, a minimum distance of 0m and a maximum of 25m are specified, being converted all values greater than the upper limit to it.
4.5. Datasets description
The datasets are built upon the sequences described above, but we need a way to feed up our models with a sense of time. For this purpose, the inputs in the datasets will be created with one or more previous rows of the sequences they include to add this temporary sense to the model in the form of a sliding temporal window.
Throughout the document we will talk about the concept moment. We will say that the moment ti is the row of data that occurred i/10 seconds ago, and therefore t0 is the data corresponding to the present moment. Figure 10 shows a simplified schematic outlining this concept.

We will define four datasets implementing different moments. All the datasets will have the same three outputs (i.e. change left, no change and change right) but a different number of inputs according to the temporal window. Table 5 describes the datasets used during the first stage of the experiment. The datasets are named dsit and dsiv, correspond to the i-th dataset for the training and validation partition respectively, and all of them correspond to the profile Sm. The dataset named dsEv is a single validation dataset and corresponds to a random selection of sequences among the ones in profile Se. The reason of having only one validation dataset for profile Se is due to the results obtained in stage 1 (see below in results section 5).

Name
No. of ←
No change
No. of →
Inputs
Moments
Size
Profile
ds1t
9804
44376
9804
4322
t5
63984
Sm
ds1v
410
1016
410
4322
t5
1836
Sm
ds2t
8544
42408
8544
4322
t5
59496
Sm
ds2v
312
976
312
4322
t5
1600
Sm
ds3t
8544
42408
8544
6483
t5, t10
59496
Sm
ds3v
312
976
312
6483
t5, t10
1600
Sm
ds4t
6504
38800
6504
8644
t5, t10, t20
51808
Sm
ds4v
144
896
144
8644
t5, t10, t20
1184
Sm
dsEv
151
725
151
8644
t5, t10, t20
1184
Se
Table 5. Datasets used for the experiments.
The choice of 5 and 10 frames (500ms and 1s according with the frequency of 10Hz) as key time points in the temporal windows is not arbitrary. Here we assume that the process of lane-change execution is a task that involves the visual cortex and the prefrontal cortex in the brain. Thus, we could assume that the response time is between 0.2s to 1.2s ([28]). 

Descripción del proceso de sincronización. Todos los dispositivos han sido, o bien configurados para una captura a 10Hz de frecuencia, o bien remuestreados a ésta.

La tabla~\ref{tbl:data-obtained-from-instrumented-vehicle} resume los datos recogidos y su dominio.

\begin{table*}[t]
	\caption[Resúmen de información extraída del vehículo instrumentado][1.5em]{Valores capturados por el vehículo instrumentado y sus dominios. Los valores de 0, 1 y 2 se corresponden con los cambios de carril, siendo 0 cambio a la izquierda, 1 no cambio y 2 a la derecha.}
	\label{tbl:data-obtained-from-instrumented-vehicle}
	\begin{tabular}{lll}
		\toprule
		Variable & Descripción & Dominio \\
		\midrule
		LiDAR & Coordenadas de todos los puntos capturados por el dispositivo. & $[-200m, 200m] \subset \mathbb{R}$ \\
		\bottomrule
	\end{tabular}
\end{table*}

\section{Entrenamiento de modelos}

\subsection{Perspectiva general}

\subsection{Comportamiento longitudinal}

\paragraph{Free flow}

\paragraph{Car following}

\paragraph{Lane exit}

\subsection{Comportamiento en cambio de carril}

\paragraph{Toma de decisión de cambio de carril}

\paragraph{Ejecución de cambio de carril}

\section{Validación del modelo}






----------------------------------------------------------------------

\TODO{Esto lo saco del paper de modelling blablabla de lane execution. Lo pongo aquí porque lcreo que lo suyo es poner la metodologías entera aquí. Además el tema de instrumentación, los recorridos, etcétera esta bien que estén aquí englobados. Hay que tener cuidado de, si se pone una cronología, que sea cierta, es decir, primero lane execution, luego lane intention, ...}

Para entrenar, comparar y validar los diferentes modelos de aceptación de cambio de carril, se ha seguido el siguiente método. Se utiliza un vehículo instrumentado para registrar los datos de conducción, primero, para el reconocimiento de patrones de conducción para clasificar los controladores en dos subconjuntos y segundo, para la construcción de conjuntos de datos para el proceso de formación de modelos. En el segundo caso, un operador estará presente en el vehículo y le pedirá a los sujetos que realicen un cambio a la izquierda o a la derecha en diferentes situaciones (principalmente con y sin automóviles en el entorno) mientras se graban los datos. Esos eventos luego se registran como intención de cambio de carril porque los sujetos deben ejecutar el cambio de carril si es posible. De esta forma, garantizamos que cada ejecución de cambio de carril (o imposibilidad de ejecución) está directamente relacionada con una intención de cambio de carril.

Las secciones que siguen describen cómo se instrumenta el vehículo, y se obtienen datos, cómo se procesan a una representación adecuada para entrenar a los modelos y, finalmente, cómo se entrena a los modelos.


\section{Perfiles de conducción}

Para tener suficiente información con la que trabajar, se han usado datos pertenecientes a conjuntos de conductores con diferentes perfiles de conducción. Todos los sujetos empleados en el experimento tiene el mismo rango de edad y género.

Tras obtener sus datos en rutas de prueba preliminares en entorno urbano, los sujetos se separaron en dos conjuntos de conductores con perfiles de conducción diferentes tal y como se describe en~\cite{DiazAlvarez2014}. Los sujetos cuyos datos eran difícilmente caracterizables no fueron incluidos en ninguno de los conjuntos.

Los parámetros usados para la clasificación corresponden al promedio y las varianzas de los 7 indicadores (uno para la velocidad, dos para la aceleración y cuatro para los tirones) para un total de 14 indicadores. Los valores se han normalizado para resaltar la diferencia entre ambos perfiles de la siguiente manera:

\begin{itemize}
	\item \textbf{Velocidad}. Media normalizada en el intervalo $[0, 20]$ y varianza en el intervalo $[0, 400]$.
	\item \textbf{Aceleración} y \textbf{jerk}. Media y varianza en el intervalo $[0, 2]$.
\end{itemize}

Los valores brutos se representan en la Tabla~\ref{tbl:raw-indicators-from-drivers-profiles}. Sus perfiles se han extraído de los datos de GPS y CAN Bus como se describe en [1]. La figura muestra el perfil normalizado para los datos de los dos subconjuntos de controladores (llamados Sm y Se).

\begin{table}
	\caption[Indicadores de los datos extraídos de cada perfil]{Indicadores extraídos a partir de los datos de cada perfil de conducción.}
	\label{tbl:raw-indicators-from-drivers-profiles}
	\begin{tabular}{@{}lllll@{}}
		\toprule
		\multirow{2}{*}{Indicador} & \multicolumn{2}{c}{$S_m$} & \multicolumn{2}{c}{$S\_e$} \\
		& $\mu$ & $\sigma$ & $\mu$ & $\sigma$ \\ \midrule
		Speed & 17.75 & 349.31 & 17.96 & 307.25 \\
		Negative acceleration (NA) & 1.91 & 4.44 & 1.52 & 1.91 \\
		Positive acceleration (PA) & 1.73 & 3.68 & 1.37 & 1.87 \\
		Starting movement jerk (SMJ) & 1.66 & 3.50 & 1.20 & 1.69 \\
		Cruising track jerk (CTJ) & 1.61 & 2.34 & 1.21 & 0.99 \\
		Starting brake jerk (SBJ) & 1.60 & 2.20 & 1.18 & 1.49 \\
		Ending brake jerk (EBJ) & 1.59 & 2.40 & 1.30 & 2.00 \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{figure}
	\centering
	\subfloat[]{\includegraphics[width=.45\textwidth]{raw-indicators-from-drivers-profiles-a}}\qquad
	\subfloat[]{\includegraphics[width=.45\textwidth]{raw-indicators-from-drivers-profiles-b}}
	\caption{Comparación de indicadores entre los diferentes perfiles de conducción.}
	\label{fig:raw-indicators-from-drivers-profiles}
\end{figure}

\section{Routes}

Para la captura de datos se prepararon dos rutas, $R_1$ para utilizar los datos extraídos como datos de entrenamiento de los modelos y $R_2$ como datos para el proceso de validación de los mismos. La Figura~\ref{fig:proposed-routes} muestra los mapas para éstas. Ambas son rutas urbanas con una duración de aproximadamente 20 minutos, con tramos de uno y dos carriles, y en las que las velocidades máximas permitidas oscilan entre los 20 y los 50 \si{\kilo\metre\per\hour}. Fueron realizadas entre las 11:00am y las 12:00pm en días laborables, permitiendo una circulación con suficientes vehículos para conducir, pero sin demasiados como para impedir la circulación.

\begin{figure}
	\centering
	\subfloat[]{\includegraphics[width=.45\textwidth]{route-1}}\qquad
	\subfloat[]{\includegraphics[width=.45\textwidth]{route-2}}
	\caption{Las dos rutas del experimento, (a) Ruta $R_1$ para entrenamiento y (b) Ruta $R_2$ para validación.}
	\label{fig:proposed-routes}
\end{figure}

Los sujetos realizaron la ruta primero para familiarizarse con el circuito. Posteriormente, las rutas fueron realizadas y los datos extraídos para el resto del proceso.

\section{Proceso de los datos}

Tras la captura, los datos fueron preprocesados antes de entrenar los modelos. Los datos de cada modelo fueron preprocesados de manera distinta, por lo que este proceso quedará descrito más adelante en las secciones dedicadas a éstos.

\subsection{Descripción de los conjuntos de datos}

The datasets are built upon the sequences described above, but we need a way to feed up our models with a sense of time. For this purpose, the inputs in the datasets will be created with one or more previous rows of the sequences they include to add this temporary sense to the model in the form of a sliding temporal window.

Throughout the document we will talk about the concept moment. We will say that the moment $t_i$ is the row of data that occurred $\frac{i}{10}$ seconds ago, and therefore $t_0$ is the data corresponding to the present moment. Figure~\ref{fig:explanation-of-data-frequency} shows a simplified schematic outlining this concept.

\begin{figure}
	\includegraphics{explanation-of-data-frequency}
	\caption{Given the row of data in time $t$, we define the moment $T_i$ as the row of data that occurred in time $t - \frac{i}{10}$ seconds.}
	\label{fig:explanation-of-data-frequency}
\end{figure}

We will define four datasets implementing different moments. All the datasets will have the same number of outputs (3 being change left, no change and change right) but a different number of inputs according to the temporal window. Table~\ref{tbl:data-sets-description} describes the datasets used during the first stage of the experiment. The datasets are named $dsi_t$ and $dsi_v$, correspond to the $i$-th dataset for the training and validation partition respectively, and all of them correspond to the profile $S_m$. The dataset named $dsE_v$ is a single validation dataset and corresponds to the profile $S_e$. The reason of having only one validation dataset for profile $S_e$ is due to the results obtained in stage 1 (see below in results section 5).

\begin{table}[t]
	\caption[Resúmen de información extraída del vehículo instrumentado][1.5em]{Descripción de los conjuntos de datos utilizados en los experimentos.}
	\label{tbl:data-sets-description}
	\begin{tabular}{llllllll}
		\toprule
		Name & left & no change & right & inputs & moments & size & profile \\
		\midrule
		$ds1_t$ & 9804 & 44376 & 9804 & 4322 & $t_5$ & 63984 & $S_m$ \\
		$ds1_v$ & 410 & 1016 & 410 & 4322 & $t_5$ & 63984 & $S_m$ \\
		$ds2_t$ & 8544 & 42408 & 8544 & 4322 & $t_{10}$ & 63984 & $S_m$ \\
		$ds2_v$ & 312 & 976 & 312 & 4322 & $t_{10}$ & 63984 & $S_m$ \\
		$ds3_t$ & 8544 & 42408 & 8544 & 6483 & $t_5, t_{10}$ & 63984 & $S_m$ \\
		$ds3_v$ & 312 & 976 & 312 & 6483 & $t_5, t_{10}$ & 63984 & $S_m$ \\
		$ds4_t$ & 6504 & 38800 & 6504 & 8644 & $t_5, t_{10}, t_{20}$ & 63984 & $S_m$ \\
		$ds4_v$ & 144 & 896 & 144 & 8644 & $t_5, t_{10}, t_{20}$ & 63984 & $S_m$ \\
		$dsE_v$ & 151 & 725 & 151 & 8644 & $t_5, t_{10}, t_{20}$ & 63984 & $S_m$ \\
		\bottomrule
	\end{tabular}
\end{table}

The choice of 5 and 10 frames (500ms and 1s according with the frequency of 10Hz) as key time points in the temporal windows is not arbitrary. Here we assume that the process of lane-change execution is a task that involves the visual cortex and the prefrontal cortex in the brain. Thus, we could assume that the response time is between 0.2s to 1.2s \cite{lipton2015critical}.

All the training and validations have been executed on a computer different from the one in the vehicle using the library TensorFlow [28] from Google. The operative system is a Debian GNU/Linux 9.1 (Stretch) over an Intel(R) Core(TM) i7-6700K CPU at 4.00GHz de 16GB of RAM and a nVidia TITAN X with 12GB of GDDR5X RAM and 3585 CUDA cores at 1.53GHz.
We have used the training method called Adam (Adaptive Momentum Estimator [29]), a very successful gradient descent algorithm which combines the idea of momentum with the RMSProp [30]. Adam algorithm relies in 4 parameters, , ,  and . The parameter  is associated to the momentum, while  is the one associated with RMSProp.   corresponds to a very small value to avoid divisions by zero and the last one, , is the learning rate. In our case, we will maintain the default parameters proposed in the paper for ,  and  (,  and  respectively) and set a learning rate  deduced by a trial and error process.
To avoid memory exhaustion problems and follow closely the evolution of the training processes, a minibatch of 1.000 items is implemented.
For the purposes of including the lane-change intention features in the CNN architectures, a modification over the general layout has been implemented. In it, the LIDAR images are presented to the network input whereas the lane-change intention features are normalized and presented to the dense layers right after all the pattern detection phase. That is the reason of the variation on the number of neurons. The intuition behind this decision is that, after all the pattern detection phase, it is expected to have some of them recognized right before the dense layers, i.e. the classification phase. We can consider lane-change intention features as already recognized patterns and thus it is not necessary to add a transformation of them into the pattern recognition phase. So, if there are N neurons specified in that layers and the dataset specifying M lane-change intention features, the dense layer is composed of  neurons.
To find the optimal architecture for this purpose, the training process is separated into two stages. In the first one (stage 1), a shallow analysis was performed over a superficial study on a wide range of architectures for all the training sets. Specifically, the training process has been performed over 135 ANNs (90 MLPs and 45CNNs as described in Error: no se encontró el origen de la referencia and Error: no se encontró el origen de la referencia respectively). The networks were trained during 10.000 epochs of 1.000 mini-batches and then validated for each of the proposed datasets (described in Table ).
In the second one (stage 2), a more in-depth study over the most interesting architectures for both families is performed, improving the deficiencies detected (as shown in the results section) and training the models until the validation error is stabilized.

-----------------------------------------------------------------

\TODO Creo que habría que indicar en la introducción y luego aquí que queremos reproducir nuestro modelo en SUMO, que determina los cambios de carril como teleportaciones de un carril a otro.

Broadly speaking, the lane-change problem within the cognitive scheme is associated with the tactical level (also manoeuvre level) on a three-layer scheme where the tasks of intermediate cognitive process are grouped.



\section{Determinando la intencionalidad en el cambio de carril}
\label{s:lane-change-intention}

\section{Ejecucutando el cambio de carril}
\label{s:lane-change-execution}


Methodology

In order to train, compare and validate the different lane-change acceptance models, the following method has been followed. An instrumented vehicle is used for recording driving data, firstly for driving patterns recognition to classify drivers into two subsets and, secondly, for constructing datasets for the models training process. In the second case, an operator will be present in the vehicle and will ask to the subjects to execute a left or right change in different situations (mainly with and without cars in the surroundings) while data is being recorded. Those events are then logged as lane-change intention because the subjects must execute the lane-change if possible. In this way, we guarantee that each lane-change execution (or impossibility of execution) is directly linked to a lane-change intention.
The sections that follow describes the how the vehicle is instrumented, and data are obtained, how they are processed to a suitable representation to train the models and, finally, how the models are trained.

Both Lane change intention and Lane change action are variables captured through the driver’s communication to an operator located in the co-pilot’s seat so some sort of noise is expected in the starting and ending point on each of the sequences of lane change manoeuvres. Experimental results have shown that a disposition where the left change and right change are in opposite sides (i.e. with the value of no change in the middle) speeds up the training convergence.


Multilayer Perceptrons

In a MLP, the neurons are arranged in layers so that all the neuron outputs on one layer are the inputs for all the neurons in the next layer. The first and last layers are called input layer and output layer respectively. The inner layers are called hidden layers. As shown in Figure 2 (b) (a two-layered MLP), in this architecture the inputs are usually presented as vectors.
As they have been used extensively in several areas with great success, we use them here to make a comparison of the improvement of the use of CNNs over MLPs.
Error: no se encontró el origen de la referencia depicts the final MLP architectures used in this work. Each number on the “architecture” column represents the number of neurons in each of the hidden layers. The output layers contain 3 neurons corresponding to the activation neurons for left-change, no action and right change. Also, each row represents a set of topologies, where  points out the dataset employed to train this architecture (described in Table  later in section 4. Methodology) and  symbolizes the dropout applied to all their hidden layers. This results in a total of 90 MLP networks to work with.
Name 1
Layers
Architecture
MLP1--


MLP2--


MLP3--


MLP4--


MLP5--


MLP6--


1 The names represent different networks depending on the dataset used for training and the dropout rate.
Table 1. MLP networks used in this work

CNN

Three architectures, each of them with different dropout rates, have been used in the comparison and are shown in Error: no se encontró el origen de la referencia.
Name 1
Layers
Architecture
CNN1--


CNN2--


CNN3--


1 The names represent different networks depending on the dataset used for training and the dropout rate.
Table 2. CNN networks used in this work.
Each element in the “architecture” column corresponds to a different kind of layer, being  a convolution layer of C channels and  size,  a max-pool layer of  size with a step of  and  a dense layer of  neurons. In our case, the input layers of a convolution operation are padded with zeros to maintain the same  size on the output. As with the MLP architectures, the output layer contains 3 neurons corresponding to the activation neurons for left-change, no action and right change and each row represents a set of topologies, where  points to the dataset employed to train this architecture (described in Table ) and  symbolizes the dropout applied to all their hidden layers. This results in a total of 45 CNN networks to work with.



Once all the data have been logged, the training and validation sets will be drawn from the intervals during the driving in which there has been a lane-change intention, regardless of whether it has been executed. Then, an exploratory training process (we call it stage 1) will be performed against the models exposed in section 3 with the different temporal versions of the sequences extracted from route  by  and will be validated against the validation sets extracted from route  by  and . After this, the best datasets and models will be chosen for a more in-deep training stage.


The data logged for both  and  contains several information about the environment in no-intention situations. This information is not relevant for the purposes of the study and therefore those data are removed from the data sets. Situations in roundabouts and crossroads are also discarded. The remaining data are then grouped in sequences of continuous events and treated separately since each of them are ordered sets with full temporally meaning.
After that, and before creating the full training and validation datasets, two more processes are accomplished to augment the available data and making it suitable for the ANNs.


Data augmentation
Complex problems require lots of data for the training process. Otherwise, they may succumb to the problem known as overfitting, where the model is capable of learning almost every example in the training data but fails in generalizing examples not seen before.
Our problem seems to fit with this description. Even more, the numbers of possible lane changes executed during driving is quite small. It is therefore necessary to artificially increase the training data set (in validation sets, data augmentation makes no sense). In our case we will use the technique of symmetry or mirroring and a technique developed for this problem that we have called shaking.
In the process of mirroring, we assume that the cognitive mechanisms that drive the lane change execution towards one side in a situation are the same that drives the execution to the opposite side in a mirrored situation (with respect to the  plane). An example of this concept is illustrated in Figure .

(a)

(b)
Figure 7. An example of the mirroring technique: (a) A regular frame captured from the lidar; (b) The same frame after the mirroring process.
By augmenting data with mirroring, the number of examples is doubled with the advantage of not losing any precision in the data.
In the technique we have called shaking, we take advantage of the imprecise nature of the LIDAR data in three-dimensional space. Considering an accuracy of ±3cm, we have a sphere of imprecision of 3cm of radius for each point in which we can situate it randomly. The name shaking refers as if we shake all the points of a frame (Figure ).
The process is as follows. For each sequence we get all the frames and, for each of them, we go through each point and apply it a new random shift of . After this process, a new sequence is created with the same lane-change intention and action, and similar but slightly different environment to the original one.

(a)

(b)
Figure 8. An example of two frames after a shaking process over the frame displayed in Figure : (a) Shaking of ; (b) Shaking of .
The intuition that this solution can help reducing the variance of the problem is that we assume that high precision is not required and that, therefore, two similar point clouds will produce similar effects. Due the LiDAR imprecision, we can play with all the 3cm spaces around each point without, in theory, lose any precision.
In our case, new sequences are generated for each original and mirrored frame. For each point of the frame, its position is shifted  being ,  and  a uniform value belonging to the open interval  or , depending on the stage of the training process.
4.4.2. Data representation
These sequences contain, apart from the intention and execution of lane changes of the driver, the surroundings of the vehicle as a point cloud. The problem here lies in the requirement of the ANNs that need the input as a fixed set of elements, and the number of points in a point cloud varies depending on the number of elements in the surrounding where the laser impacts. For this solution a representation as a 2D deepness map is used.

In this sense, the data acquired by our laser scanner is transformed into the image domain. Thus, the distance information of each point is projected into a 2D matrix where the vertical axis corresponds to the elevation angles and the horizontal axis is the azimuth angles. The former is discretized between -12 and 3 degrees with a step of 2.5 degrees, while the azimuth values corresponds to a 360 degrees wide field of view, discretized with a step of 1 degree. This configuration allows the transformation of all the point clouds into images with the same resolution of $6 \times 360$ (Figure~\ref{fig:deepness-map}).

\begin{figure}
	\includegraphics{deepness-map}
	\caption{Un ejemplo de mapa de profundidad capturado en una de las pruebas. La celda es más azul cuanto más cercano está el obstáculo detectado.}
	\label{fig:deepness-map}
\end{figure}

One problem that occurs using LiDAR sensors is that the divergence between points grows with the distance to them. Due this divergence in the data, the interval for the vertical field of view has been selected considering the sparse data acquired with high values of the elevation angles.
All the values in the deepness map are normalized to [0, 1]. For this, a minimum distance of 0m and a maximum of 25m are specified, if all the events over this distance are not relevant for this problem (lane-change acceptance in an urban environment).