\chapter{Modelo de comportamiento de conductor}
\label{ch:behavior-model}

\section{Introducción}

\TODO{no sé qué poner}

\TODO{Hablar del esquema general (a muy grandes rasgos) del modelo de conducción que se pretende conseguir. Por qué es así, por qué queremos esas salidas y qué limitaciones nos impone el simulador sobre el que trabajamos. Aunque SUMO es continuo en el espacio, vamos sobre raíles.}

El modelo será entrenado siguiendo un esquema supervisado, por lo que necesitamos datos reales a partir de los cuales deberá ajustar su funcionamiento.

La información que se considera suficiente para que los modelos desempeñen su función en el entorno de simulación y la razón por la cual se ha tenido en cuenta es la siguiente:

\begin{itemize}
	\item \textbf{Entorno} El conductor, y por tanto el modelo, desarrolla su actividad y basa sus comportamientos, entre otros factores, en el entorno en el que se encuentra inmerso. Se considera por tanto que el ajuste de la velocidad y, sobre todo los cambios de carril, se ven influenciados por éste.
	\item \textbf{Velocidad actual del vehículo y velocidad máxima del carril}. Tanto la velocidad en la vía como del vehículo en un momento concreto influye en cómo el conductor va a modificar su aceleración en momentos posteriores.
	\item \textbf{Distancia y diferencia de velocidad con el vehículo delantero}. Al igual que con la velocidad, el vehículo delantero juega un papel esencial en los cambios de aceleración. También se intuye que puede influir en el comportamiento de cambio de carril en casos en los que el vehículo delantero va muy lento
	\item \textbf{Siguiente salida y carriles cortados}. Se considera que este tipo de información es crucial a la hora de realizar cambios de carril, ya que además de lo obvio, puede influir en otras maniobras tales como un adelantamiento.
	\item \textbf{Señales}. Es interesante contar con este tipo de señales ya que pueden influir en diferentes patrones de aceleración (e.g. estamos cerca y cambia de color a ámbar) o desaceleración (e.g. aproximación a un ceda el paso, stop o semáforo en rojo).
\end{itemize}

\section{Extracción de datos de conducción}

Se ha hecho uso de un vehículo instrumentado con sensores para capturar la mayor cantidad posible de información especificada en la introducción. El vehículo en cuestión es un Mitsubishi iMiEV (Figura~\ref{fig:instrumented-imiev}) y los dispositivos un LIDAR, un GPS, el Bus CAN y una cámara de los cuales hablamos a continuación.

\begin{figure}
	\includegraphics{instrumented-imiev}
	\caption{El vehículo utilizado en los ensayos realizados. Se trata de un Mitsubishi iMiEV instrumentado con un LIDAR anclado en la baca superior, un GPS anclado en el techo, un puerto de acceso directo al Bus CAN y una cámara Microsoft Kinect tras el espejo retrovisor.}
	\label{fig:instrumented-imiev}
\end{figure}

\paragraph{LIDAR}

\TODO{Descripción detallada de qué es el lidar. Marca y cositas. Algo del estilo mide la distancia a través la diferencia entre la emisión de pulsos de luz y su reflejo en un sensor. A lo mejor queda bien en un sidenote. El usado es un Velodyne VLP-16, un LiDAR circular de 16 canales verticales separados 2º, dando un FOV (-15, 15) grados de apertura vertical). Este dispositivo se conecta a la máquina a través del puerto ethernet y se usa para la obtención de información del entorno del conductor. El LiDAR está situado en el techo del vehículo orientando los (0, 0) grados en al dirección y sentido de éste y con el plano horizontal paralelo al suelo.}

\TODO{Descripción de los datos que se extraen y cómo se almacenan.}

\paragraph{GPS}

\TODO{Yo creo que se podría decir que se usa para obtener una segunda medida de velocidad y aceleración, así como para la localización espacial de subconjuntos de datos interesantes para su estudio.}

\TODO{Descripción de los datos que se extraen y cómo se almacenan. Indicar los dos tipos de mensaje que se obtienen!}

\paragraph{Bus CAN}

\TODO{Explicar qué es el CAN Bus, y si es necesario, un poquitín su funcionamiento (aunque sea en un sidenote, así queda todo más completito.). El BUS del vehículo se conecta a través del puerto USB al ordenador a lo mejor hay que explicar un poco más la conexión, el tipo de cable y tal. Es usado para la extracción de la interacción del conductor con el vehículo.}

\TODO{Descripción de los datos que se extraen y cómo se almacenan.}

\paragraph{Cámara}

\TODO{Descripción de la cámara}

\TODO{Descripción de los datos que se extraen y cómo se almacenan.}

\subsection{Almacenamiento de datos}

Todos los dispositivos se conectan a un ordenador con sistema operativo Ubuntu GNU/Linux sobre Intel i7-7500U CPU con 16GB de memoria RAM. El esquema del vehículo instrumentado se detalla en la Figura~\ref{fig:instrumented-imiev-schema}.

\begin{figure}
	\includegraphics{instrumented-imiev-schema}
	\caption{The instrumented vehicle schema.}
	\label{fig:instrumented-imiev-schema}
\end{figure}

\TODO{Descripción del proceso de captura, los nodos de ROS y el nodo principal.

\subsection{Selección de rutas}

\subsection{Selección de sujetos}

\section{Preparación de los datos}

Descripción del proceso de sincronización. Todos los dispositivos han sido, o bien configurados para una captura a 10Hz de frecuencia, o bien remuestreados a ésta.

La tabla~\ref{tbl:data-obtained-from-instrumented-vehicle} resume los datos recogidos y su dominio.

\begin{table*}[t]
	\caption[Resúmen de información extraída del vehículo instrumentado][1.5em]{Valores capturados por el vehículo instrumentado y sus dominios. Los valores de 0, 1 y 2 se corresponden con los cambios de carril, siendo 0 cambio a la izquierda, 1 no cambio y 2 a la derecha.}
	\label{tbl:data-obtained-from-instrumented-vehicle}
	\begin{tabular}{lll}
		\toprule
		Variable & Descripción & Dominio \\
		\midrule
		LiDAR & Coordenadas de todos los puntos capturados por el dispositivo. & $[-200m, 200m] \subset \mathbb{R}$ \\
		\bottomrule
	\end{tabular}
\end{table*}

\section{Entrenamiento de modelos}

\subsection{Perspectiva general}

\subsection{Comportamiento longitudinal}

\paragraph{Free flow}

\paragraph{Car following}

\paragraph{Lane exit}

\subsection{Comportamiento en cambio de carril}

\paragraph{Toma de decisión de cambio de carril}

\paragraph{Ejecución de cambio de carril}

\section{Validación del modelo}






----------------------------------------------------------------------

\TODO{Esto lo saco del paper de modelling blablabla de lane execution. Lo pongo aquí porque lcreo que lo suyo es poner la metodologías entera aquí. Además el tema de instrumentación, los recorridos, etcétera esta bien que estén aquí englobados. Hay que tener cuidado de, si se pone una cronología, que sea cierta, es decir, primero lane execution, luego lane intention, ...}

Para entrenar, comparar y validar los diferentes modelos de aceptación de cambio de carril, se ha seguido el siguiente método. Se utiliza un vehículo instrumentado para registrar los datos de conducción, primero, para el reconocimiento de patrones de conducción para clasificar los controladores en dos subconjuntos y segundo, para la construcción de conjuntos de datos para el proceso de formación de modelos. En el segundo caso, un operador estará presente en el vehículo y le pedirá a los sujetos que realicen un cambio a la izquierda o a la derecha en diferentes situaciones (principalmente con y sin automóviles en el entorno) mientras se graban los datos. Esos eventos luego se registran como intención de cambio de carril porque los sujetos deben ejecutar el cambio de carril si es posible. De esta forma, garantizamos que cada ejecución de cambio de carril (o imposibilidad de ejecución) está directamente relacionada con una intención de cambio de carril.

Las secciones que siguen describen cómo se instrumenta el vehículo, y se obtienen datos, cómo se procesan a una representación adecuada para entrenar a los modelos y, finalmente, cómo se entrena a los modelos.


\section{Perfiles de conducción}

Para tener suficiente información con la que trabajar, se han usado datos pertenecientes a conjuntos de conductores con diferentes perfiles de conducción. Todos los sujetos empleados en el experimento tiene el mismo rango de edad y género.

Tras obtener sus datos en rutas de prueba preliminares en entorno urbano, los sujetos se separaron en dos conjuntos de conductores con perfiles de conducción diferentes tal y como se describe en~\cite{DiazAlvarez2014}. Los sujetos cuyos datos eran difícilmente caracterizables no fueron incluidos en ninguno de los conjuntos.

Los parámetros usados para la clasificación corresponden al promedio y las varianzas de los 7 indicadores (uno para la velocidad, dos para la aceleración y cuatro para los tirones) para un total de 14 indicadores. Los valores se han normalizado para resaltar la diferencia entre ambos perfiles de la siguiente manera:

\begin{itemize}
	\item \textbf{Velocidad}. Media normalizada en el intervalo $[0, 20]$ y varianza en el intervalo $[0, 400]$.
	\item \textbf{Aceleración} y \textbf{jerk}. Media y varianza en el intervalo $[0, 2]$.
\end{itemize}

Los valores brutos se representan en la Tabla~\ref{tbl:raw-indicators-from-drivers-profiles}. Sus perfiles se han extraído de los datos de GPS y CAN Bus como se describe en [1]. La figura muestra el perfil normalizado para los datos de los dos subconjuntos de controladores (llamados Sm y Se).

\begin{table}
	\caption[Indicadores de los datos extraídos de cada perfil]{Indicadores extraídos a partir de los datos de cada perfil de conducción.}
	\label{tbl:raw-indicators-from-drivers-profiles}
	\begin{tabular}{@{}lllll@{}}
		\toprule
		\multirow{2}{*}{Indicador} & \multicolumn{2}{c}{$S_m$} & \multicolumn{2}{c}{$S\_e$} \\
		& $\mu$ & $\sigma$ & $\mu$ & $\sigma$ \\ \midrule
		Speed & 17.75 & 349.31 & 17.96 & 307.25 \\
		Negative acceleration (NA) & 1.91 & 4.44 & 1.52 & 1.91 \\
		Positive acceleration (PA) & 1.73 & 3.68 & 1.37 & 1.87 \\
		Starting movement jerk (SMJ) & 1.66 & 3.50 & 1.20 & 1.69 \\
		Cruising track jerk (CTJ) & 1.61 & 2.34 & 1.21 & 0.99 \\
		Starting brake jerk (SBJ) & 1.60 & 2.20 & 1.18 & 1.49 \\
		Ending brake jerk (EBJ) & 1.59 & 2.40 & 1.30 & 2.00 \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{figure}
	\centering
	\subfloat[]{\includegraphics[width=.47\textwidth]{raw-indicators-from-drivers-profiles-a}}\qquad
	\subfloat[]{\includegraphics[width=.47\textwidth]{raw-indicators-from-drivers-profiles-b}}
	\caption{Comparación de indicadores entre los diferentes perfiles de conducción.}
	\label{fig:raw-indicators-from-drivers-profiles}
\end{figure}

\section{Routes}

Para la captura de datos se prepararon dos rutas, $R_1$ para utilizar los datos extraídos como datos de entrenamiento de los modelos y $R_2$ como datos para el proceso de validación de los mismos. La Figura~\ref{fig:proposed-routes} muestra los mapas para éstas. Ambas son rutas urbanas con una duración de aproximadamente 20 minutos, con tramos de uno y dos carriles, y en las que las velocidades máximas permitidas oscilan entre los 20 y los 50 \si{\kilo\metre\per\hour}. Fueron realizadas entre las 11:00am y las 12:00pm en días laborables, permitiendo una circulación con suficientes vehículos para conducir, pero sin demasiados como para impedir la circulación.

\begin{figure}
	\centering
	\subfloat[]{\includegraphics[width=.45\textwidth]{route-1}}\qquad
	\subfloat[]{\includegraphics[width=.45\textwidth]{route-2}}
	\caption{Las dos rutas del experimento, (a) Ruta $R_1$ para entrenamiento y (b) Ruta $R_2$ para validación.}
	\label{fig:proposed-routes}
\end{figure}

Los sujetos realizaron la ruta primero para familiarizarse con el circuito. Posteriormente, las rutas fueron realizadas y los datos extraídos para el resto del proceso.

\section{Proceso de los datos}

Tras la captura, los datos fueron preprocesados antes de entrenar los modelos. Los datos de cada modelo fueron preprocesados de manera distinta, por lo que este proceso quedará descrito más adelante en las secciones dedicadas a éstos.

\subsection{Descripción de los conjuntos de datos}

The datasets are built upon the sequences described above, but we need a way to feed up our models with a sense of time. For this purpose, the inputs in the datasets will be created with one or more previous rows of the sequences they include to add this temporary sense to the model in the form of a sliding temporal window.

Throughout the document we will talk about the concept moment. We will say that the moment $t_i$ is the row of data that occurred $\frac{i}{10}$ seconds ago, and therefore $t_0$ is the data corresponding to the present moment. Figure~\ref{fig:explanation-of-data-frequency} shows a simplified schematic outlining this concept.

\begin{figure}
	\includegraphics{explanation-of-data-frequency}
	\caption{Given the row of data in time $t$, we define the moment $T_i$ as the row of data that occurred in time $t - \frac{i}{10}$ seconds.}
	\label{fig:explanation-of-data-frequency}
\end{figure}

We will define four datasets implementing different moments. All the datasets will have the same number of outputs (3 being change left, no change and change right) but a different number of inputs according to the temporal window. Table~\ref{tbl:data-sets-description} describes the datasets used during the first stage of the experiment. The datasets are named $dsi_t$ and $dsi_v$, correspond to the $i$-th dataset for the training and validation partition respectively, and all of them correspond to the profile $S_m$. The dataset named $dsE_v$ is a single validation dataset and corresponds to the profile $S_e$. The reason of having only one validation dataset for profile $S_e$ is due to the results obtained in stage 1 (see below in results section 5).

\begin{table}[t]
	\caption[Resúmen de información extraída del vehículo instrumentado][1.5em]{Descripción de los conjuntos de datos utilizados en los experimentos.}
	\label{tbl:data-sets-description}
	\begin{tabular}{llllllll}
		\toprule
		Name & left & no change & right & inputs & moments & size & profile \\
		\midrule
		$ds1_t$ & 9804 & 44376 & 9804 & 4322 & $t_5$ & 63984 & $S_m$ \\
		$ds1_v$ & 410 & 1016 & 410 & 4322 & $t_5$ & 63984 & $S_m$ \\
		$ds2_t$ & 8544 & 42408 & 8544 & 4322 & $t_{10}$ & 63984 & $S_m$ \\
		$ds2_v$ & 312 & 976 & 312 & 4322 & $t_{10}$ & 63984 & $S_m$ \\
		$ds3_t$ & 8544 & 42408 & 8544 & 6483 & $t_5, t_{10}$ & 63984 & $S_m$ \\
		$ds3_v$ & 312 & 976 & 312 & 6483 & $t_5, t_{10}$ & 63984 & $S_m$ \\
		$ds4_t$ & 6504 & 38800 & 6504 & 8644 & $t_5, t_{10}, t_{20}$ & 63984 & $S_m$ \\
		$ds4_v$ & 144 & 896 & 144 & 8644 & $t_5, t_{10}, t_{20}$ & 63984 & $S_m$ \\
		$dsE_v$ & 151 & 725 & 151 & 8644 & $t_5, t_{10}, t_{20}$ & 63984 & $S_m$ \\
		\bottomrule
	\end{tabular}
\end{table}

The choice of 5 and 10 frames (500ms and 1s according with the frequency of 10Hz) as key time points in the temporal windows is not arbitrary. Here we assume that the process of lane-change execution is a task that involves the visual cortex and the prefrontal cortex in the brain. Thus, we could assume that the response time is between 0.2s to 1.2s \cite{lipton2015critical}.

All the training and validations have been executed on a computer different from the one in the vehicle using the library TensorFlow [28] from Google. The operative system is a Debian GNU/Linux 9.1 (Stretch) over an Intel(R) Core(TM) i7-6700K CPU at 4.00GHz de 16GB of RAM and a nVidia TITAN X with 12GB of GDDR5X RAM and 3585 CUDA cores at 1.53GHz.
We have used the training method called Adam (Adaptive Momentum Estimator [29]), a very successful gradient descent algorithm which combines the idea of momentum with the RMSProp [30]. Adam algorithm relies in 4 parameters, , ,  and . The parameter  is associated to the momentum, while  is the one associated with RMSProp.   corresponds to a very small value to avoid divisions by zero and the last one, , is the learning rate. In our case, we will maintain the default parameters proposed in the paper for ,  and  (,  and  respectively) and set a learning rate  deduced by a trial and error process.
To avoid memory exhaustion problems and follow closely the evolution of the training processes, a minibatch of 1.000 items is implemented.
For the purposes of including the lane-change intention features in the CNN architectures, a modification over the general layout has been implemented. In it, the LIDAR images are presented to the network input whereas the lane-change intention features are normalized and presented to the dense layers right after all the pattern detection phase. That is the reason of the variation on the number of neurons. The intuition behind this decision is that, after all the pattern detection phase, it is expected to have some of them recognized right before the dense layers, i.e. the classification phase. We can consider lane-change intention features as already recognized patterns and thus it is not necessary to add a transformation of them into the pattern recognition phase. So, if there are N neurons specified in that layers and the dataset specifying M lane-change intention features, the dense layer is composed of  neurons.
To find the optimal architecture for this purpose, the training process is separated into two stages. In the first one (stage 1), a shallow analysis was performed over a superficial study on a wide range of architectures for all the training sets. Specifically, the training process has been performed over 135 ANNs (90 MLPs and 45CNNs as described in Error: no se encontró el origen de la referencia and Error: no se encontró el origen de la referencia respectively). The networks were trained during 10.000 epochs of 1.000 mini-batches and then validated for each of the proposed datasets (described in Table ).
In the second one (stage 2), a more in-depth study over the most interesting architectures for both families is performed, improving the deficiencies detected (as shown in the results section) and training the models until the validation error is stabilized.

-----------------------------------------------------------------

\TODO Creo que habría que indicar en la introducción y luego aquí que queremos reproducir nuestro modelo en SUMO, que determina los cambios de carril como teleportaciones de un carril a otro.

Broadly speaking, the lane-change problem within the cognitive scheme is associated with the tactical level (also manoeuvre level) on a three-layer scheme where the tasks of intermediate cognitive process are grouped.



\section{Determinando la intencionalidad en el cambio de carril}
\label{s:lane-change-intention}

\section{Ejecucutando el cambio de carril}
\label{s:lane-change-execution}


Methodology

In order to train, compare and validate the different lane-change acceptance models, the following method has been followed. An instrumented vehicle is used for recording driving data, firstly for driving patterns recognition to classify drivers into two subsets and, secondly, for constructing datasets for the models training process. In the second case, an operator will be present in the vehicle and will ask to the subjects to execute a left or right change in different situations (mainly with and without cars in the surroundings) while data is being recorded. Those events are then logged as lane-change intention because the subjects must execute the lane-change if possible. In this way, we guarantee that each lane-change execution (or impossibility of execution) is directly linked to a lane-change intention.
The sections that follow describes the how the vehicle is instrumented, and data are obtained, how they are processed to a suitable representation to train the models and, finally, how the models are trained.

Both Lane change intention and Lane change action are variables captured through the driver’s communication to an operator located in the co-pilot’s seat so some sort of noise is expected in the starting and ending point on each of the sequences of lane change manoeuvres. Experimental results have shown that a disposition where the left change and right change are in opposite sides (i.e. with the value of no change in the middle) speeds up the training convergence.


Multilayer Perceptrons

In a MLP, the neurons are arranged in layers so that all the neuron outputs on one layer are the inputs for all the neurons in the next layer. The first and last layers are called input layer and output layer respectively. The inner layers are called hidden layers. As shown in Figure 2 (b) (a two-layered MLP), in this architecture the inputs are usually presented as vectors.
As they have been used extensively in several areas with great success, we use them here to make a comparison of the improvement of the use of CNNs over MLPs.
Error: no se encontró el origen de la referencia depicts the final MLP architectures used in this work. Each number on the “architecture” column represents the number of neurons in each of the hidden layers. The output layers contain 3 neurons corresponding to the activation neurons for left-change, no action and right change. Also, each row represents a set of topologies, where  points out the dataset employed to train this architecture (described in Table  later in section 4. Methodology) and  symbolizes the dropout applied to all their hidden layers. This results in a total of 90 MLP networks to work with.
Name 1
Layers
Architecture
MLP1--


MLP2--


MLP3--


MLP4--


MLP5--


MLP6--


1 The names represent different networks depending on the dataset used for training and the dropout rate.
Table 1. MLP networks used in this work

CNN

Three architectures, each of them with different dropout rates, have been used in the comparison and are shown in Error: no se encontró el origen de la referencia.
Name 1
Layers
Architecture
CNN1--


CNN2--


CNN3--


1 The names represent different networks depending on the dataset used for training and the dropout rate.
Table 2. CNN networks used in this work.
Each element in the “architecture” column corresponds to a different kind of layer, being  a convolution layer of C channels and  size,  a max-pool layer of  size with a step of  and  a dense layer of  neurons. In our case, the input layers of a convolution operation are padded with zeros to maintain the same  size on the output. As with the MLP architectures, the output layer contains 3 neurons corresponding to the activation neurons for left-change, no action and right change and each row represents a set of topologies, where  points to the dataset employed to train this architecture (described in Table ) and  symbolizes the dropout applied to all their hidden layers. This results in a total of 45 CNN networks to work with.



Once all the data have been logged, the training and validation sets will be drawn from the intervals during the driving in which there has been a lane-change intention, regardless of whether it has been executed. Then, an exploratory training process (we call it stage 1) will be performed against the models exposed in section 3 with the different temporal versions of the sequences extracted from route  by  and will be validated against the validation sets extracted from route  by  and . After this, the best datasets and models will be chosen for a more in-deep training stage.


The data logged for both  and  contains several information about the environment in no-intention situations. This information is not relevant for the purposes of the study and therefore those data are removed from the data sets. Situations in roundabouts and crossroads are also discarded. The remaining data are then grouped in sequences of continuous events and treated separately since each of them are ordered sets with full temporally meaning.
After that, and before creating the full training and validation datasets, two more processes are accomplished to augment the available data and making it suitable for the ANNs.


Data augmentation
Complex problems require lots of data for the training process. Otherwise, they may succumb to the problem known as overfitting, where the model is capable of learning almost every example in the training data but fails in generalizing examples not seen before.
Our problem seems to fit with this description. Even more, the numbers of possible lane changes executed during driving is quite small. It is therefore necessary to artificially increase the training data set (in validation sets, data augmentation makes no sense). In our case we will use the technique of symmetry or mirroring and a technique developed for this problem that we have called shaking.
In the process of mirroring, we assume that the cognitive mechanisms that drive the lane change execution towards one side in a situation are the same that drives the execution to the opposite side in a mirrored situation (with respect to the  plane). An example of this concept is illustrated in Figure .

(a)

(b)
Figure 7. An example of the mirroring technique: (a) A regular frame captured from the lidar; (b) The same frame after the mirroring process.
By augmenting data with mirroring, the number of examples is doubled with the advantage of not losing any precision in the data.
In the technique we have called shaking, we take advantage of the imprecise nature of the LIDAR data in three-dimensional space. Considering an accuracy of ±3cm, we have a sphere of imprecision of 3cm of radius for each point in which we can situate it randomly. The name shaking refers as if we shake all the points of a frame (Figure ).
The process is as follows. For each sequence we get all the frames and, for each of them, we go through each point and apply it a new random shift of . After this process, a new sequence is created with the same lane-change intention and action, and similar but slightly different environment to the original one.

(a)

(b)
Figure 8. An example of two frames after a shaking process over the frame displayed in Figure : (a) Shaking of ; (b) Shaking of .
The intuition that this solution can help reducing the variance of the problem is that we assume that high precision is not required and that, therefore, two similar point clouds will produce similar effects. Due the LiDAR imprecision, we can play with all the 3cm spaces around each point without, in theory, lose any precision.
In our case, new sequences are generated for each original and mirrored frame. For each point of the frame, its position is shifted  being ,  and  a uniform value belonging to the open interval  or , depending on the stage of the training process.
4.4.2. Data representation
These sequences contain, apart from the intention and execution of lane changes of the driver, the surroundings of the vehicle as a point cloud. The problem here lies in the requirement of the ANNs that need the input as a fixed set of elements, and the number of points in a point cloud varies depending on the number of elements in the surrounding where the laser impacts. For this solution a representation as a 2D deepness map is used.

In this sense, the data acquired by our laser scanner is transformed into the image domain. Thus, the distance information of each point is projected into a 2D matrix where the vertical axis corresponds to the elevation angles and the horizontal axis is the azimuth angles. The former is discretized between -12 and 3 degrees with a step of 2.5 degrees, while the azimuth values corresponds to a 360 degrees wide field of view, discretized with a step of 1 degree. This configuration allows the transformation of all the point clouds into images with the same resolution of $6 \times 360$ (Figure~\ref{fig:deepness-map}).

\begin{figure}
	\includegraphics{deepness-map}
	\caption{Un ejemplo de mapa de profundidad capturado en una de las pruebas. La celda es más azul cuanto más cercano está el obstáculo detectado.}
	\label{fig:deepness-map}
\end{figure}

One problem that occurs using LiDAR sensors is that the divergence between points grows with the distance to them. Due this divergence in the data, the interval for the vertical field of view has been selected considering the sparse data acquired with high values of the elevation angles.
All the values in the deepness map are normalized to [0, 1]. For this, a minimum distance of 0m and a maximum of 25m are specified, if all the events over this distance are not relevant for this problem (lane-change acceptance in an urban environment).